# Bias, Harms, and Transparency

> Recognizing problematic AI outputs and setting appropriate expectations with users.

## TL;DR

- LLMs inherit biases from training data—they can produce **discriminatory, offensive, or factually wrong** outputs
- For user-facing features: assess **human impact** before deploying AI
- Be transparent: **disclose AI involvement**, provide citations, communicate limitations
- Some domains require extra safeguards: **health, finance, legal, hiring, content moderation**
- Know when to say no: some workflows shouldn't use AI

## Core Concepts

### Where Bias Comes From

LLMs learn patterns from training data. If that data contains biases—and it does—the model reproduces them.

**Types of bias:**

| Type | Example |
|------|---------|
| **Representation bias** | Training data over-represents certain demographics, under-represents others |
| **Stereotyping** | Associating professions with genders ("nurse" → female, "engineer" → male) |
| **Cultural bias** | Defaulting to Western/English perspectives |
| **Temporal bias** | Reflecting attitudes from when training data was collected |
| **Selection bias** | Training on easily-available internet text skews toward certain viewpoints |

**Practical example:**

If you ask AI to generate sample user data:
- Names might skew toward certain ethnicities
- Job titles might correlate with gender stereotypically
- Age distributions might not reflect your actual user base

### Recognizing Harmful Outputs

Not all problems are obvious. Watch for:

**1. Subtle discrimination**

Example: Asking AI to "Write a job description for a software engineer" might produce output with gendered language ("he will...", "guys on the team") or list requirements that inadvertently exclude certain groups.

**2. Factual misinformation**

Example: Asking "Explain how to handle user authentication" might produce output suggesting outdated or insecure practices, presented with confidence that looks authoritative.

**3. Inappropriate content generation**

User prompts processed through your app could manipulate the model into generating harmful content, making your app the delivery mechanism.

**4. Stereotyped recommendations**

Example: "Suggest products for this user profile" might produce recommendations based on stereotyped assumptions rather than actual user behavior or preferences.

### Human Impact Assessment

Before deploying AI in user-facing features, assess:

**1. Who is affected?**
- All users equally?
- Certain groups disproportionately?
- Vulnerable populations?

**2. What decisions does the AI influence?**
- Informational only?
- Recommendations users might act on?
- Automated decisions affecting users?

**3. What's the failure mode?**
- Wrong answer is annoying but harmless?
- Wrong answer causes financial loss?
- Wrong answer causes psychological harm?
- Wrong answer leads to physical harm?

**4. Can users identify and correct errors?**
- Is AI output labeled as AI-generated?
- Can users override or appeal?
- Is there a human escalation path?

### Risk Tiers

| Tier | Impact | Examples | Required Safeguards |
|------|--------|----------|---------------------|
| **Low** | Inconvenience | Code completion, text formatting, internal tools | Basic filtering |
| **Medium** | Meaningful impact | Customer service, content recommendations, search | Human review paths, clear disclosures |
| **High** | Significant harm potential | Health advice, financial guidance, hiring | Human oversight required, extensive testing |
| **Critical** | Safety/legal | Medical diagnosis, legal advice, credit decisions | Generally avoid AI automation; regulatory requirements |

### User Transparency

For any user-facing AI feature, transparency is non-negotiable.

**1. Disclose AI involvement**

```typescript
// Bad: Hidden AI
const response = await getAnswer(userQuestion);
return response; // User thinks human wrote this

// Good: Disclosed
return {
  answer: response,
  metadata: {
    source: 'ai-generated',
    model: 'gpt-4o',
    disclaimer: 'This response was generated by AI and may contain errors.',
  },
};
```

**2. Provide citations when possible**

```typescript
// If using RAG or source documents
return {
  answer: response,
  sources: [
    { title: 'Official Documentation', url: 'https://...', excerpt: '...' },
    { title: 'Support Article', url: 'https://...', excerpt: '...' },
  ],
};
```

**3. Communicate limitations clearly**

```typescript
const disclaimers = {
  general: 'AI-generated content may be inaccurate. Verify important information.',
  codeGeneration: 'Generated code may contain bugs or security issues. Review before use.',
  dataAnalysis: 'Analysis is based on patterns in data and may not reflect causal relationships.',
};
```

**4. Show uncertainty when appropriate**

```typescript
// Model outputs with confidence indicators
interface AIResponse {
  answer: string;
  confidence: 'high' | 'medium' | 'low';
  caveats: string[];
}

// UI renders low-confidence answers differently
if (response.confidence === 'low') {
  renderWithWarning(response, 'This answer has low confidence. Consider consulting additional sources.');
}
```

### High-Stakes Domains

Some domains require extra care:

**Healthcare**
- Never position AI as medical advice
- Recommend professional consultation
- Consider regulatory requirements (FDA, HIPAA)

**Finance**
- Financial advice has legal implications
- Disclosures may be legally required
- Historical data doesn't guarantee future results

**Legal**
- AI cannot provide legal advice
- Jurisdiction matters significantly
- Stakes of wrong information are high

**Hiring/HR**
- High potential for discrimination
- Regulatory scrutiny (EEOC, local laws)
- Impact on people's livelihoods

**Content Moderation**
- False positives silence legitimate speech
- False negatives allow harmful content
- Context is crucial and often lost

### When Not to Use AI

Some workflows shouldn't use AI, or should use it only with human oversight:

| Workflow | Concern | Recommendation |
|----------|---------|----------------|
| Final hiring decisions | Discrimination risk | AI can assist, human decides |
| Loan approvals | Regulatory and fairness | Human review required |
| Medical treatment plans | Safety critical | Professional oversight mandatory |
| Content removal | Free speech impact | Human appeal process |
| User bans | Account impact | Human review for permanent actions |

## In Practice

### Bias Testing

Build bias detection into your evaluation:

```typescript
// Test for demographic parity
async function testForBias(prompt: string, variations: Record<string, string>) {
  const results: Record<string, string> = {};

  for (const [label, modifiedPrompt] of Object.entries(variations)) {
    results[label] = await generateResponse(modifiedPrompt);
  }

  // Compare outputs for unexpected differences
  return analyzeForDifferentialTreatment(results);
}

// Example usage
await testForBias('Write a recommendation letter for {name}', {
  neutral: 'Write a recommendation letter for Alex',
  male: 'Write a recommendation letter for James',
  female: 'Write a recommendation letter for Emily',
  // Compare: Are there tone differences? Different qualities emphasized?
});
```

### Building Safeguards

```typescript
// Content filtering layer
async function safeGenerate(prompt: string, options: SafetyOptions): Promise<SafeResponse> {
  // Pre-generation checks
  if (containsHighRiskTopic(prompt, options.blockedTopics)) {
    return { blocked: true, reason: 'Topic outside safe operating area' };
  }

  const response = await generate(prompt);

  // Post-generation checks
  if (containsHarmfulContent(response)) {
    return { blocked: true, reason: 'Generated content failed safety filter' };
  }

  if (options.requireHumanReview && meetsReviewThreshold(response)) {
    return { content: response, status: 'pending_review' };
  }

  return { content: response, status: 'approved' };
}
```

## Common Pitfalls

- **Assuming neutrality.** Models have biases. Testing is required.
- **Hiding AI involvement.** Users deserve to know.
- **No escalation path.** When AI fails, users need recourse.
- **Testing only happy paths.** Edge cases reveal biases.

## Related

- [Operational Guardrails](./operational-guardrails.md) — Day-to-day safety practices
- [Moderation and Policy](../04-shipping-ai-features/moderation-policy.md) — Production content filtering
- [Evals Basics](./evals-basics.md) — Testing AI behavior
